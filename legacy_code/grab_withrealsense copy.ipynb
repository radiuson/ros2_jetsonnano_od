{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#coding=utf-8\n",
    "import time\n",
    "from Arm_Lib import Arm_Device\n",
    "import numpy as np\n",
    "import ikpy.chain\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import platform\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import (\n",
    "    check_img_size, non_max_suppression, apply_classifier, scale_coords, \n",
    "    xyxy2xywh, plot_one_box, strip_optimizer, set_logging)\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "SERVO_SPEED = 3\n",
    "LINK_LIST = [0,30,83,83,80,90]\n",
    "TOMATO_SIZE = (0.033, 0.037)  # (单位：米)\n",
    "CAMERA_ROT_AXIS = [0.577, 0.577, 0.577]  # z 轴\n",
    "GRABBER = {\n",
    "    'open':20,\n",
    "    'close':180,\n",
    "}\n",
    "DROP_POSITION = [0,-0.2,0.15]\n",
    "\n",
    "def servo_read(Arm):\n",
    "    angle = []\n",
    "    time.sleep(0.02)\n",
    "    for i in range(6):\n",
    "        aa = Arm.Arm_serial_servo_read(i+1)\n",
    "        if aa:\n",
    "            angle.append(aa)\n",
    "        else:\n",
    "            angle.append(0)\n",
    "        time.sleep(.002)\n",
    "    time.sleep(.002)\n",
    "    return angle\n",
    "    \n",
    "\n",
    "\n",
    "def servo_write(Arm,angle,servo_speed,s_time=None):\n",
    "    calculate_time = calculate_servotime(Arm,angle,servo_speed)\n",
    "    # s_time = 1500\n",
    "    if s_time:\n",
    "        Arm.Arm_serial_servo_write6(angle[0], angle[1], angle[2], angle[3], angle[4], angle[5], s_time)\n",
    "        time.sleep(s_time/1000)\n",
    "        return s_time\n",
    "    else:\n",
    "        Arm.Arm_serial_servo_write6(angle[0], angle[1], angle[2], angle[3], angle[4], angle[5], calculate_time)\n",
    "        time.sleep(calculate_time/1000)\n",
    "        return calculate_time\n",
    "    \n",
    "\n",
    "def calculate_servotime(Arm,target,servo_speed=3):\n",
    "    servotime = np.array(servo_read(Arm))-np.array(target)\n",
    "    return int(max(max(np.abs(servotime)) *servo_speed*5,500))\n",
    "\n",
    "def detect(save_img=False):\n",
    "    out, source, weights, view_img, save_txt, imgsz = 'inference/output', '0', '/home/jetson/code/yolov5/yolov5-3.0/1105.pt', True, True, 320\n",
    "    webcam = source == '0' or source.startswith('rtsp') or source.startswith('http') or source.endswith('.txt')\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device('')\n",
    "    if os.path.exists(out):\n",
    "        shutil.rmtree(out)  # delete output folder\n",
    "    os.makedirs(out)  # make new output folder\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n",
    "        modelc.to(device).eval()\n",
    "\n",
    "\n",
    "    # Get names and colors\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
    "\n",
    "    # Run inference\n",
    "    t0 = time.time()\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "    return model\n",
    "\n",
    "def one_step(model,pipeline,names,img_save=True):\n",
    "    \n",
    "    frames = pipeline.wait_for_frames()\n",
    "    aligned_frames = align.process(frames)\n",
    "    aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "    color_frame = aligned_frames.get_color_frame()\n",
    "    # 将帧转换为numpy数组\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "    depth_intrinsics = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "    device = select_device('')\n",
    "    # path, img, im0s, vid_cap = LoadStreams(source, img_size=320)\n",
    "    \n",
    "    img = torch.tensor(color_image)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    \n",
    "    img = img.to(device)\n",
    "    t1 = time_synchronized()\n",
    "    pred = model(img, augment=True)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, 0.8, 0.6, classes=None, agnostic=True)\n",
    "    t2 = time_synchronized()\n",
    "\n",
    "    # Process detections\n",
    "    img = np.array(img.squeeze(0).cpu())\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        # Print time (inference + NMS)\n",
    "        print('Done. ({:.3f}s)'.format((t2 - t1)))    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if det is not None:\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            label = '%s %.2f' % (names[int(cls)], conf)\n",
    "            print(xyxy)\n",
    "            plot_one_box(xyxy, img, label=label, line_thickness=3)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    if img_save:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(\"a.png\",img)\n",
    "    return det,aligned_depth_frame,depth_intrinsics\n",
    "\n",
    "\n",
    "\n",
    "def camera_to_world(P_cam, T_cam, R_cam):\n",
    "    \"\"\"\n",
    "    将相机坐标系下的点转换为世界坐标系。\n",
    "    \n",
    "    参数：\n",
    "        P_cam (numpy.array): 相机坐标系下的点 (3x1)。\n",
    "        T_cam (numpy.array): 相机在世界坐标系中的位置 (3x1)。\n",
    "        R_cam (numpy.array): 相机的旋转矩阵 (3x3)。\n",
    "    \n",
    "    返回：\n",
    "        numpy.array: 世界坐标系下的点 (3x1)。\n",
    "    \"\"\"\n",
    "    P_world = R_cam @ P_cam + T_cam\n",
    "    return P_world\n",
    "\n",
    "def calculate_frame_coor(det):\n",
    "    res = []\n",
    "    for i,(*xyxy, conf, cls) in enumerate(det):\n",
    "        # 现在只抓番茄\n",
    "        if cls == 1:\n",
    "            x = int(xyxy[2]-xyxy[0])\n",
    "            y = int(xyxy[3]-xyxy[1])\n",
    "            c_x = int((xyxy[0]+ xyxy[2])/2)\n",
    "            c_y = int((xyxy[1]+ xyxy[3])/2)\n",
    "            res.append([i,c_x,c_y,x,y,conf,cls])\n",
    "    return res\n",
    "\n",
    "def select_tomato():\n",
    "    # 根据坐标计算得分并选择番茄,现在是用得分最高的\n",
    "    # 可以用最靠近一侧的番茄\n",
    "    pass\n",
    "\n",
    "def control_arm_coor(my_chain,target_position,grabber):\n",
    "    converted_position = np.array(target_position)\n",
    "    print(\"target coor:\",converted_position)\n",
    "    joints = my_chain.inverse_kinematics(converted_position,initial_position = np.radians([0,90,130,20,40,90,30]))\n",
    "    joint_list = joints[1:-1]\n",
    "    print(\"servos should be\",np.degrees(joint_list))\n",
    "    joint_deg_list = [int(x)for x in np.degrees(joint_list)]\n",
    "    joint_deg_list.append(grabber)\n",
    "    print(\"The angles of each joints should be:\" , joint_deg_list)\n",
    "    real_frame = my_chain.forward_kinematics(joints)\n",
    "    error = np.abs(np.linalg.norm(list(real_frame[:3,3]),ord=2)-np.linalg.norm(converted_position,ord=2))\n",
    "    print(\"Error:{:.2f}%\".format(error*100))\n",
    "    print(\"The position is:\\n\", real_frame)\n",
    "    if \"{:.2f}%\".format(error*0.01) != \"0.00%\":\n",
    "        print(\"out of range\")\n",
    "    else:\n",
    "        return servo_write(Arm,joint_deg_list,SERVO_SPEED)\n",
    "    \n",
    "def object_position(det,aligned_depth_frame,depth_intrinsics):\n",
    "    res = calculate_frame_coor(det)\n",
    "    point_list = []\n",
    "    for obj in res:\n",
    "        depth_value = aligned_depth_frame.get_distance(obj[1], obj[2])\n",
    "        point = rs.rs2_deproject_pixel_to_point(depth_intrinsics, [obj[1], obj[2]], depth_value)\n",
    "        point = [-point[2],point[0],point[1]]\n",
    "        point_list.append(point)\n",
    "        print(f\"Index:{obj[0]}, Pixel: ({obj[1]}, {obj[2]}) -> Point: {point}\")\n",
    "    # 这里的pcam用的是置信度最高的目标\n",
    "    point_list = np.array(point_list)\n",
    "    # filtered_point_list = point_list[point_list[:, 0] < 0]\n",
    "    filtered_point_list = point_list[point_list[:, 0]!=0]\n",
    "\n",
    "    sorted_point_list = filtered_point_list[np.argsort(filtered_point_list[:, 0])]\n",
    "    try:\n",
    "        P_cam = sorted_point_list[0]\n",
    "    except:\n",
    "        return None\n",
    "    # 计算旋转矩阵\n",
    "    T_cam,R_cam = camera_position(Arm,my_chain)\n",
    "\n",
    "    # 将相机坐标系的点转换为世界坐标系\n",
    "    P_world = camera_to_world(P_cam, T_cam, R_cam)\n",
    "    print(\"Position of the most confident object to be tomato :\", np.array(P_world))\n",
    "    return np.array(P_world)\n",
    "\n",
    "def cam_rotation_matrix(axis, translation, angle_deg = 10):\n",
    "    \"\"\"\n",
    "    计算绕任意轴旋转的齐次变换矩阵。\n",
    "\n",
    "    参数：\n",
    "        axis (list or np.array): 旋转轴的方向向量，例如 [1, 0, 0] 表示 x 轴。\n",
    "        translation: translation(coordinate)\n",
    "        angle_deg (float): 旋转角度（以度为单位，正值为逆时针旋转）。\n",
    "    \n",
    "    返回：\n",
    "        np.array: 4x4 的齐次旋转矩阵。\n",
    "    \"\"\"\n",
    "    # 归一化旋转轴\n",
    "    axis = np.array(axis)\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    x, y, z = axis\n",
    "\n",
    "    # 将角度从度转换为弧度\n",
    "    angle_rad = np.radians(angle_deg)\n",
    "    c = np.cos(angle_rad)\n",
    "    s = np.sin(angle_rad)\n",
    "    t = 1 - c\n",
    "\n",
    "    # 构造旋转矩阵 (3x3 部分)\n",
    "    R = np.array([\n",
    "        [t*x*x + c,   t*x*y - s*z, t*x*z + s*y],\n",
    "        [t*x*y + s*z, t*y*y + c,   t*y*z - s*x],\n",
    "        [t*x*z - s*y, t*y*z + s*x, t*z*z + c]\n",
    "    ])\n",
    "\n",
    "    # 构造齐次变换矩阵 (4x4)\n",
    "    rotation_matrix = np.eye(4)\n",
    "    rotation_matrix[:3, :3] = R\n",
    "    # rotation_matrix[3, :3] = [[translation[0]],[translation[1]],[translation[2]]]\n",
    "    rotation_matrix[:3, 3] = translation\n",
    "    return rotation_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def camera_position(Arm,my_chain):\n",
    "    servos = servo_read(Arm)\n",
    "    servos = np.radians([0,servos[0],servos[1],servos[2],servos[3],servos[4],servos[5]])\n",
    "    transformations = my_chain.forward_kinematics(servos, full_kinematics=True)\n",
    "    servo4_transform = transformations[4]\n",
    "    \n",
    "    print(f\"关节 {4} 的变换: {servo4_transform},\\n坐标:{servo4_transform[:3,3]}\")\n",
    "    camera_transform = np.dot(servo4_transform, cam_rotation_matrix(CAMERA_ROT_AXIS,[0.05,-0.06,-0.033],120))\n",
    "    camera_transform = np.dot(camera_transform,np.array([\n",
    "        [1,0,0,0],\n",
    "        [0,-1,0,0],\n",
    "        [0,0,-1,0],\n",
    "        [0,0,0,1]\n",
    "    ]) )\n",
    "    camera_transform = np.dot(camera_transform,np.array([\n",
    "        [1,0,0,0],\n",
    "        [0,-1,0,0],\n",
    "        [0,0,1,0],\n",
    "        [0,0,0,1]\n",
    "    ]) )\n",
    "    camera_transform = np.dot(camera_transform, cam_rotation_matrix([0,0,0],[0,0,0],0))\n",
    "    camera_position = camera_transform[:3, 3]\n",
    "    camera_rotation = camera_transform[:3, :3]\n",
    "    print(\"camera position:\", camera_position)\n",
    "    return camera_position,camera_rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_chain = ikpy.chain.Chain.from_urdf_file(\"/home/jetson/code/yolov5/yolov5-3.0/arm_real copy.URDF\")\n",
    "Arm = Arm_Device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_angle = [90,90,90,90,90,150]\n",
    "target_angle = [90,130,40,10,90,150]\n",
    "servo_write(Arm,target_angle,SERVO_SPEED,1500)\n",
    "actual_angle = servo_read(Arm)\n",
    "print(f\"target angle:{target_angle},\\nactual servo angle:{actual_angle}\")\n",
    "# object_position(det,aligned_depth_frame,depth_intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化RealSense管道\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "# 启动管道\n",
    "pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_arm_coor(my_chain,[0,-0.07,0.23],GRABBER['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = [135, 96, 29, 22, 90, 150]\n",
    "# servo_write(Arm,target,SERVO_SPEED)\n",
    "model = attempt_load('/home/jetson/code/yolov5/yolov5-3.0/1205.pt', map_location=select_device('')).half()  # load FP32 model\n",
    "# Run inference\n",
    "img = torch.zeros((1, 3, 640, 640), device=select_device(''))  # init img\n",
    "_ = model(img.half())\n",
    "names = ['stem','tomato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = [[0,-0.07,0.23]]\n",
    "s_time = control_arm_coor(my_chain,task_list[0],GRABBER['open'])\n",
    "\n",
    "while True:\n",
    "    target = task_list[0]\n",
    "    s_time = control_arm_coor(my_chain,target,GRABBER['open'])\n",
    "    time.sleep(1)\n",
    "    flag = True\n",
    "    det,aligned_depth_frame,depth_intrinsics = one_step(model,pipeline,names)\n",
    "\n",
    "    while det is not None and flag :\n",
    "        flag = False\n",
    "        position_tomato = object_position(det,aligned_depth_frame,depth_intrinsics)\n",
    "        if position_tomato is not None:\n",
    "            print(position_tomato)\n",
    "            if control_arm_coor(my_chain,position_tomato,GRABBER['open']):\n",
    "                control_arm_coor(my_chain,position_tomato,GRABBER['close'])\n",
    "                control_arm_coor(my_chain,DROP_POSITION,GRABBER['close'])\n",
    "                control_arm_coor(my_chain,DROP_POSITION,GRABBER['open'])\n",
    "                control_arm_coor(my_chain,target,GRABBER['open'])\n",
    "                flag = True\n",
    "        time.sleep(1)\n",
    "        det,aligned_depth_frame,depth_intrinsics = one_step(model,pipeline,names)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Task on {task_list.pop(0)} is finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ikpy.chain\n",
    "from Arm_Lib import Arm_Device\n",
    "Arm = Arm_Device()\n",
    "my_chain = ikpy.chain.Chain.from_urdf_file(\"/home/jetson/code/yolov5/yolov5-3.0/arm_real copy.URDF\")\n",
    "degree_list = [0,90, 90, 50, 40, 90, 179]\n",
    "servo_write(Arm,degree_list[1:],SERVO_SPEED,s_time = 2000)\n",
    "angle_test = np.radians(degree_list)\n",
    "transformations = my_chain.forward_kinematics(angle_test, full_kinematics=True)\n",
    "for i in range(1, len(transformations)):  # 忽略起点（索引0）\n",
    "    print(f\"关节 {i} 的位置: {transformations[i][:3, 3]}\")\n",
    "joint_list = list(my_chain.inverse_kinematics(transformations[-1][:3,3],initial_position=angle_test))\n",
    "\n",
    "print(joint_list)\n",
    "print(\"origin\",degree_list)\n",
    "print(\"solve\",[int(x) for x in np.degrees(joint_list)])\n",
    "real_frame = my_chain.forward_kinematics(joint_list, full_kinematics=True)\n",
    "for i in range(1, len(real_frame)):  # 忽略起点（索引0）\n",
    "    print(f\"关节 {i} 的位置: {real_frame[i][:3, 3]}\")\n",
    "\n",
    "print(\"real servo angles:\",read_servolines(Arm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = list(my_chain.inverse_kinematics([-0.05,0,0.35],initial_position=angle_test))\n",
    "print(np.degrees(joint_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_position = [-0.05,0,0.35]\n",
    "grabber = 20\n",
    "converted_position = np.array(target_position)\n",
    "print(\"target coor:\",converted_position)\n",
    "joints = my_chain.inverse_kinematics(converted_position,initial_position = np.radians([0,90,130,20,40,90,30]))\n",
    "joint_list = joints[1:-1]\n",
    "print(\"servos should be\",np.degrees(joint_list))\n",
    "joint_deg_list = [int(x)for x in np.degrees(joint_list)]\n",
    "joint_deg_list.append(grabber)\n",
    "print(\"The angles of each joints should be:\" , joint_deg_list)\n",
    "real_frame = my_chain.forward_kinematics(joints)\n",
    "error = np.abs(np.linalg.norm(list(real_frame[:3,3]),ord=2)-np.linalg.norm(converted_position,ord=2))\n",
    "print(\"Error:{:.2f}%\".format(error*100))\n",
    "print(\"The position is:\\n\", real_frame)\n",
    "if \"{:.2f}%\".format(error*100) != \"0.00%\":\n",
    "    print(\"out of range\")\n",
    "else:\n",
    "    servo_write(Arm,joint_deg_list,SERVO_SPEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = detect()\n",
    "names = ['stem','tomato']\n",
    "det,aligned_depth_frame,depth_intrinsics = one_step(model,pipeline,names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
